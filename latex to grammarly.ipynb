{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\\begin{anythin}\n",
    "\\item \\textbf{PCA}\n",
    " -- based on the covariance matrix of\n",
    " \n",
    "\\section{Conclusions} \\label{SEC:conclusions}\n",
    "\n",
    "Our experiments show that when tackling time series data of enormous size one should consider dimensionality reduction methods based both on selection and on transformation. There is no clear indication whether to use selection- or transformation-based models. At least some of the analyzed dimensionality reduction models can be effectively used in real-world applications using big datasets. Logistic regression and random forests have feasible training time even for big datasets. Random forests can achieve satisfactory results even on nonlinear tasks. The problem of classifying failures seems to be such a nonlinear problem.\n",
    "\n",
    "% You can use bibtex or put bibliography in the \\thebibliography section\n",
    "\\bibliographystyle{splncs} \n",
    "\\bibliography{sample}\n",
    "\\end{document}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latex_to_grammarly(text):\n",
    "    text = text.replace('\\_', '_')\n",
    "    \n",
    "    # remove tabels\n",
    "    temp = []\n",
    "    in_table = False\n",
    "    for line in text.split('\\n'):\n",
    "        if '\\begin{table}' in line.lower():\n",
    "            in_table = True\n",
    "        elif not in_table:\n",
    "            temp.append(line)\n",
    "        elif '\\end{table}' in line.lower():\n",
    "            in_table = False\n",
    "    text = '\\n'.join(temp)\n",
    "    \n",
    "    # begin{}, end{}, comments, ref{}\n",
    "    # label{}, cite{}, citep{}, bibliographies, itemsep\n",
    "    # includegraphics\n",
    "    to_remove = ('\\\\begin{.+?}', r'\\\\end{.+?}', '%.+?\\n', '\\\\ref{.+?}',\n",
    "                 r'\\\\label{.+?}', r'\\\\citep{.+?}',\n",
    "                 '\\\\bibliographystyle{.+?}', '\\\\bibliography{.+?}', r'\\\\itemsep-.\\d+em',\n",
    "                 r'\\\\includegraphics.+?{.+?}', r'\\\\url{.+?}')\n",
    "    for pattern in to_remove:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # bold, italics, (sub)sections\n",
    "    # captions, another italics (emphasis)\n",
    "    to_remove_okalajace = ('\\\\textbf{(.+?)}', '\\\\textit{(.+?)}', r'\\\\[sub]*section{(.+?)}', \n",
    "                           r'\\\\caption{(.+?)}', r'{\\\\em(.+?)}')\n",
    "    for pattern in to_remove_okalajace:\n",
    "        text = re.sub(pattern, r'\\1', text)\n",
    "    \n",
    "    # special treatment for citep\n",
    "    to_podmienic = (r'\\\\cite{.+?}',)\n",
    "    for pattern in to_podmienic:\n",
    "        text = re.sub(pattern, 'authors', text)\n",
    "    \n",
    "    # item, --\n",
    "    text = text.replace('\\item', '-')\n",
    "    text = text.replace('--', '-')\n",
    "    \n",
    "    # mathematical expressions\n",
    "    # $\\chi^2$\n",
    "    text = re.sub('\\$.+?=.+?\\$', 'EQUALITY', text)    \n",
    "    text = re.sub('\\$.+?\\$', 'CONSTANT', text)\n",
    "    \n",
    "    # remove multiple spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub('\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # remove spaces before dots and commas which might occur\n",
    "    text = re.sub(' \\.', '.', text)\n",
    "    text = re.sub(' ,', ',', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\n",
      "- PCA\n",
      " - based on the covariance matrix of\n",
      " \n",
      "Conclusions \n",
      "\n",
      "Our experiments show that when tackling time series data of enormous size one should consider dimensionality reduction methods based both on selection and on transformation. There is no clear indication whether to use selection- or transformation-based models. At least some of the analyzed dimensionality reduction models can be effectively used in real-world applications using big datasets. Logistic regression and random forests have feasible training time even for big datasets. Random forests can achieve satisfactory results even on nonlinear tasks. The problem of classifying failures seems to be such a nonlinear problem.\n",
      "\n",
      "\b \n",
      "\b\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print latex_to_grammarly(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
